## Intent
Commit to at least 10 hours a week learning about and trying ML. 





## First Week

### Reading
We read the following papers in a journal club
* [PonderNet](https://deepmind.com/research/publications/2021/Ponder-Net)
  * follow up: understanding the parity task, trying ponder on it. 
* [Closed-Form Factorization of Latent Semantics in GANs](https://arxiv.org/pdf/2007.06600.pdf)


### People
Caught up with a friend in ML, who spoke about
* In metagenomics: language models for reference-less understanding of bacterial sequences
* Denoising auto-encoders -- see fields of digital pathology and radiology. 
* pyTorchLightning: powerful library and good tutorials

### Coursework
Working on Jeremy Howard's FastAI course, which promtped me to start this blog. What's exciting to see is how standardized best practices have become. For example, RandomResizedCrop as a function. Additionally, the naming of the concepts like
* Limited Scope Deployment
* Out of domain data (data that we haven't trained on comes in, like video images that are much harder to interpret than photos)
* Domain Shift, where the type of data that the model sees changes over time
