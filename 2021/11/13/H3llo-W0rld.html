<h2 id="intent">Intent</h2>
<p>Commit to at least 10 hours a week learning about and trying ML.</p>

<h2 id="first-week">First Week</h2>

<h3 id="reading">Reading</h3>
<p>We read the following papers in a journal club</p>
<ul>
  <li>https://deepmind.com/research/publications/2021/Ponder-Net
    <ul>
      <li>follow up: understanding the parity task,</li>
    </ul>
  </li>
  <li>https://arxiv.org/pdf/2007.06600.pdf</li>
</ul>

<h3 id="people">People</h3>
<p>Caught up with a friend in ML, who spoke about</p>
<ul>
  <li>In metagenomics: language models for reference-less understanding of bacterial sequences</li>
  <li>Denoising auto-encoders – see fields of digital pathology and radiology.</li>
  <li>pyTorchLightning: powerful library and good tutorials</li>
</ul>

<h3 id="coursework">Coursework</h3>
<p>Working on Jeremy Howard’s FastAI course, which promtped me to start this blog. What’s exciting to see is how standardized best practices have become. For example, RandomResizedCrop as a function. Additionally, the naming of the concepts like</p>
<ul>
  <li>Limited Scope Deployment</li>
  <li>Out of domain data (data that we haven’t trained on comes in, like video images that are much harder to interpret than photos)</li>
  <li>Domain Shift, where the type of data that the model sees changes over time</li>
</ul>
